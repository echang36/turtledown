import nltk
from nltk.classify.util import apply_features
import string
import glob
import random


#Import liberal data sources
cb_files=glob.glob("cb/*")
dk_files=glob.glob("dk/*")
my_files=glob.glob("my/*")

#Import conservative data sources
rs_files=glob.glob("rs/*")
rwn_files=glob.glob("rwn/*")


#For folding, create a dataset that has:
#List 1-5, each of which contains a list of tuples
#Tuples contain the text of a file and whether it is liberal or conservative
random.seed(36)
overall_data_set={1:[], 2:[], 3:[], 4:[], 5:[]}
for file in cb_files:
	f=open(file,'r')
	data_point=((f.read(),"Liberal"))
	(overall_data_set[random.randint(1,5)]).append(data_point)
	f.close()

for file in dk_files:
	f=open(file,'r')
	data_point=((f.read(),"Liberal"))
	(overall_data_set[random.randint(1,5)]).append(data_point)
	f.close()

for file in my_files:
	f=open(file,'r')
	data_point=((f.read(),"Liberal"))
	(overall_data_set[random.randint(1,5)]).append(data_point)
	f.close()

for file in rs_files:
	f=open(file,'r')
	data_point=((f.read(),"Conservative"))
	(overall_data_set[random.randint(1,5)]).append(data_point)
	f.close()

for file in rwn_files:
	f=open(file,'r')
	data_point=((f.read(),"Conservative"))
	(overall_data_set[random.randint(1,5)]).append(data_point)
	f.close()

#Create training and test data points, based on which fold we are on
fold_num = 1
train_data_points=[]
test_data_points=[]
test_data_points = overall_data_set[fold_num]
gen = (i for i in overall_data_set if i != fold_num)
for i in gen:
    for item in overall_data_set[i]:
        train_data_points.append(item)


#Create proper test and training sets based on features 
train_set = apply_features(feature_extracting_function, train_data_points)
test_set = apply_features(feature_extracting_function, test_data_points)


#Run the NLTK Naive Bayes Classifier on the training set 
nb = nltk.NaiveBayesClassifier.train(train_set)

#NLTK Accuracy: Run trained model on the test set 
print "Accuracy: "+str(nltk.classify.accuracy(nb, test_set))


def feature_extracting_function(data_point):

	features={}

	data_point = nltk.clean_html(data_point)
	data_point = ''.join(ch for ch in data_point if ch not in set(string.punctuation)) #Strip punctuation

	words = data_point.split()
	words = [word.lower() for word in words] #convert all words to lowercase

	for word in words:
        if word not in nltk.corpus.stopwords.words('english'):
            features ["contains_unigrams_(%s)" %(word)] = True

    return features











